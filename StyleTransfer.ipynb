{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer\n",
    "이번 과제에서는 [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n",
    "에서 제시된 스타일 트랜스퍼 기법을 구현하는 것을 목표로 합니다.\n",
    "\n",
    "일반적인 아이디어는 두 개의 이미지를 사용해, 하나의 이미지는 콘텐츠를 반영하고 다른 하나의 이미지는 예술적 “스타일”을 반영하는 새로운 이미지를 생성하는 것을 목표로 합니다.<br>\n",
    "우리는 먼저 깊은 신경망의 특징 공간에서 각각의 이미지 콘텐츠와 스타일이 일치하도록 하는 손실 함수를 정식화한 뒤, 이미지 자체의 픽셀에 대해 경사하강을 수행합니다.<br>\n",
    "\n",
    "특징 추출기로 사용하는 딥 네트워크는 ImageNet으로 학습된 작은 모델인 SqueezeNet\n",
    "이다.<br> 어떤 네트워크든 사용할 수 있지만, 우리는 작은 크기와 효율성 때문에 SqueezeNet을 선택하여 진행합니다.\n",
    "\n",
    "다음은 이 노트북을 통해 최종적으로 만들 수 있는 이미지의 예시입니다:\n",
    "![caption](example_styletransfer.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과제의 이 부분에서는 CIFAR-10 데이터가 아니라 실제 JPEG 이미지를 다루기 때문에, 이미지를 처리하기 위한 몇 가지 헬퍼 함수를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img, size=512):\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in SQUEEZENET_STD.tolist()]),\n",
    "        T.Normalize(mean=[-m for m in SQUEEZENET_MEAN.tolist()], std=[1, 1, 1]),\n",
    "        T.Lambda(rescale),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x):\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def features_from_img(imgpath, imgsize):\n",
    "    img = preprocess(PIL.Image.open(imgpath), size=imgsize)\n",
    "    img_var = img.type(dtype)\n",
    "    return extract_features(img_var, cnn), img_var\n",
    "\n",
    "# Older versions of scipy.misc.imresize yield different results\n",
    "# from newer versions, so we check to make sure scipy is up to date.\n",
    "def check_scipy():\n",
    "    import scipy\n",
    "    vnum = int(scipy.__version__.split('.')[1])\n",
    "    major_vnum = int(scipy.__version__.split('.')[0])\n",
    "    \n",
    "    assert vnum >= 16 or major_vnum >= 1, \"You must install SciPy >= 0.16.0 to complete this notebook.\"\n",
    "\n",
    "check_scipy()\n",
    "\n",
    "answers = dict(np.load('style-transfer-checks.npz'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 과제에서처럼, CPU 또는 GPU 중 하나를 선택하기 위해 dtype을 설정해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# Uncomment out the following line if you're on a machine with a GPU set up for PyTorch!\n",
    "#dtype = torch.cuda.FloatTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/por1329/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/por1329/anaconda3/envs/gaussian_splatting/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /home/por1329/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ed633261774bef8b12ec2fa8da3695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/4.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained SqueezeNet model.\n",
    "cnn = torchvision.models.squeezenet1_1(pretrained=True).features\n",
    "cnn.type(dtype)\n",
    "\n",
    "# We don't want to train the model any further, so we don't want PyTorch to waste computation \n",
    "# computing gradients on parameters we're never going to update.\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We provide this helper code which takes an image, a model (cnn), and returns a list of\n",
    "# feature maps, one per layer.\n",
    "def extract_features(x, cnn):\n",
    "    \"\"\"\n",
    "    Use the CNN to extract features from the input image x.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, C, H, W) holding a minibatch of images that\n",
    "      will be fed to the CNN.\n",
    "    - cnn: A PyTorch model that we will use to extract features.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A list of feature for the input images x extracted using the cnn model.\n",
    "      features[i] is a PyTorch Tensor of shape (N, C_i, H_i, W_i); recall that features\n",
    "      from different layers of the network may have different numbers of channels (C_i) and\n",
    "      spatial dimensions (H_i, W_i).\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    prev_feat = x\n",
    "    for i, module in enumerate(cnn._modules.values()):\n",
    "        next_feat = module(prev_feat)\n",
    "        features.append(next_feat)\n",
    "        prev_feat = next_feat\n",
    "    return features\n",
    "\n",
    "#please disregard warnings about initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss\n",
    "\n",
    "이제 손실 함수의 세 가지 구성 요소를 계산할 것입니다.<br>\n",
    "\n",
    "손실 함수는 세 가지 항의 가중합으로 이루어진다:<br> 콘텐츠 손실 + 스타일 손실 + 총변동 손실.<br>\n",
    "\n",
    "아래에서 이러한 가중 항들을 계산하는 함수를 채워 넣게 될 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n",
    "우리는 손실 함수에 두 이미지를 모두 포함시켜, 한 이미지의 콘텐츠와 다른 이미지의 스타일을 반영하는 이미지를 생성할 수 있습니다.<br>\n",
    "목표는 콘텐츠 이미지의 콘텐츠에서 벗어나는 정도와 스타일 이미지의 스타일에서 벗어나는 정도를 모두 패널티로 주는 것입니다.<br>\n",
    "이후 이 혼합 손실 함수를 사용하여 모델 파라미터가 아닌, 초기 생성 이미지의 픽셀 값에 대해 경사하강을 수행합니다.<br>\n",
    "\n",
    "먼저 콘텐츠 손실 함수를 정의합니다.<br>\n",
    "콘텐츠 손실은 생성된 이미지의 특징 맵(feature map)이 원본 콘텐츠 이미지의 특징 맵과 얼마나 다른지를 측정합니다.<br>\n",
    "네트워크의 특정 한 레이어(예: 레이어 $\\ell$)의 콘텐츠 표현만 고려하며, 이 레이어의 특징 맵은 다음과 같습니다:\n",
    "\n",
    "$A^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$\n",
    "\n",
    "$C_\\ell$: 레이어 $\\ell$의 필터(채널) 수\n",
    "\n",
    "$H_\\ell, W_\\ell$: 특징 맵의 높이와 너비\n",
    "\n",
    "이제 공간적 위치를 하나의 차원으로 합친 리쉐이프된 특징 맵을 이용한다. 현재 생성 이미지의 특징 맵은\n",
    "\n",
    "$F^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$\n",
    "\n",
    "콘텐츠 원본 이미지의 특징 맵은\n",
    "\n",
    "$P^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$\n",
    "\n",
    "여기서 $M_\\ell = H_\\ell \\times W_\\ell$ 는 각 특징 맵의 요소 개수입니다.\n",
    "$F^\\ell$ 또는 $P^\\ell$의 각 행은 특정 필터가 이미지 전 위치에 대해 활성화된 값을 벡터로 펼친 것입니다.\n",
    "또한, 손실 함수에서 콘텐츠 손실 항의 가중치를 $w_c$라 하자.\n",
    "\n",
    "콘텐츠 손실은 다음과 같이 정의된다:\n",
    "$L_c = w_c \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "아래의 `Content loss`의 함수를 구현하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(content_weight, content_current, content_original):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - content_weight: Scalar giving the weighting for the content loss.\n",
    "    - content_current: features of the current image; this is a PyTorch Tensor of shape\n",
    "      (1, C_l, H_l, W_l).\n",
    "    - content_target: features of the content image, Tensor with shape (1, C_l, H_l, W_l).\n",
    "    \n",
    "    Returns:\n",
    "    - scalar content loss\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content loss를 테스트하시오.<br>\n",
    "오류는 0.0001보다 작게 나타나야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    image_size =  192\n",
    "    content_layer = 3\n",
    "    content_weight = 6e-2\n",
    "    \n",
    "    c_feats, content_img_var = features_from_img(content_image, image_size)\n",
    "    \n",
    "    bad_img = torch.zeros(*content_img_var.data.size()).type(dtype)\n",
    "    feats = extract_features(bad_img, cnn)\n",
    "    \n",
    "    student_output = content_loss(content_weight, c_feats[content_layer], feats[content_layer]).cpu().data.numpy()\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Maximum error is {:.3f}'.format(error))\n",
    "\n",
    "content_loss_test(answers['cl_out'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style loss\n",
    "이제 스타일 손실을 정의한다.<br>\n",
    "특정 레이어 $\\ell$에 대해 스타일 손실은 다음과 같이 정의된다.<br>\n",
    "\n",
    "먼저 해당 레이어의 특징 맵 $F$(앞서 정의한 형식과 동일)를 사용하여 Gram 행렬 $G$를 계산한다.<br>\n",
    "Gram 행렬은 각 필터의 반응 간 상관 관계를 나타내며, 이는 공분산 행렬의 근사값이다. 생성 이미지의 활성화 통계가 스타일 이미지의 활성화 통계와 일치하도록 만들고 싶기 때문에, 공분산(또는 그 근사치)을 맞추는 방식이 적합하다.<br>\n",
    "구현 방식은 여러 가지가 있을 수 있지만 Gram 행렬은 계산이 간단하고 실제로 우수한 결과를 보여 널리 사용된다.\n",
    "\n",
    "특징 맵 $F^\\ell$의 형태가 $(C_\\ell, M_\\ell)$일 때, Gram 행렬의 형태는 $(C_\\ell, C_\\ell)$이며, 원소는 다음과 같다:\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "\n",
    "여기서\n",
    "\n",
    "$G^\\ell$: 현재 이미지의 Gram 행렬\n",
    "\n",
    "$A^\\ell$: 스타일 이미지의 Gram 행렬\n",
    "\n",
    "$w_\\ell$: 레이어 $\\ell$에 대한 가중치\n",
    "\n",
    "레이어 $\\ell$의 스타일 손실은 두 Gram 행렬 간의 가중 유클리드 거리로 정의된다:\n",
    "$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "실제로는 단일 레이어 $\\ell$이 아니라 여러 레이어 집합 $\\mathcal{L}$에서 스타일 손실을 계산하는 경우가 많으며, 이때 전체 스타일 손실은 각 레이어 스타일 손실의 합이다:\n",
    "\n",
    "$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n",
    "\n",
    "## Assignment 2\n",
    "아래에서 Gram 행렬 계산을 구현하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(features, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix from features.\n",
    "    \n",
    "    Inputs:\n",
    "    - features: PyTorch Tensor of shape (N, C, H, W) giving features for\n",
    "      a batch of N images.\n",
    "    - normalize: optional, whether to normalize the Gram matrix\n",
    "        If True, divide the Gram matrix by the number of neurons (H * W * C)\n",
    "    \n",
    "    Returns:\n",
    "    - gram: PyTorch Tensor of shape (N, C, C) giving the\n",
    "      (optionally normalized) Gram matrices for the N input images.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gram 행렬 코드를 테스트하시오. 오차는 0.0001보다 작아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gram_matrix_test(correct):\n",
    "    style_image = 'styles/starry_night.jpg'\n",
    "    style_size = 192\n",
    "    feats, _ = features_from_img(style_image, style_size)\n",
    "    student_output = gram_matrix(feats[5].clone()).cpu().data.numpy()\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Maximum error is {:.3f}'.format(error))\n",
    "    \n",
    "gram_matrix_test(answers['gm_out'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "다음으로, 스타일 손실을 구현하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now put it together in the style_loss function...\n",
    "def style_loss(feats, style_layers, style_targets, style_weights):\n",
    "    \"\"\"\n",
    "    Computes the style loss at a set of layers.\n",
    "    \n",
    "    Inputs:\n",
    "    - feats: list of the features at every layer of the current image, as produced by\n",
    "      the extract_features function.\n",
    "    - style_layers: List of layer indices into feats giving the layers to include in the\n",
    "      style loss.\n",
    "    - style_targets: List of the same length as style_layers, where style_targets[i] is\n",
    "      a PyTorch Tensor giving the Gram matrix of the source style image computed at\n",
    "      layer style_layers[i].\n",
    "    - style_weights: List of the same length as style_layers, where style_weights[i]\n",
    "      is a scalar giving the weight for the style loss at layer style_layers[i].\n",
    "      \n",
    "    Returns:\n",
    "    - style_loss: A PyTorch Tensor holding a scalar giving the style loss.\n",
    "    \"\"\"\n",
    "    # Hint: you can do this with one for loop over the style layers, and should\n",
    "    # not be very much code (~5 lines). You will need to use your gram_matrix function.\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스타일 손실 구현을 테스트하시오. 오차는 0.0001보다 작아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def style_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    style_image = 'styles/starry_night.jpg'\n",
    "    image_size =  192\n",
    "    style_size = 192\n",
    "    style_layers = [1, 4, 6, 7]\n",
    "    style_weights = [300000, 1000, 15, 3]\n",
    "    \n",
    "    c_feats, _ = features_from_img(content_image, image_size)    \n",
    "    feats, _ = features_from_img(style_image, style_size)\n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
    "    \n",
    "    student_output = style_loss(c_feats, style_layers, style_targets, style_weights).cpu().data.numpy()\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Error is {:.3f}'.format(error))\n",
    "\n",
    "    \n",
    "style_loss_test(answers['sl_out'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total-variation regularization\n",
    "이미지에 매끄러움(smoothness)을 유도하는 것이 도움이 된다는 사실이 알려져 있다. 이를 위해, 픽셀 값의 흔들림(wiggles) 또는 “총변동(total variation)”을 패널티로 주는 항을 손실 함수에 추가할 수 있다.\n",
    "\n",
    "“총변동(total variation)”은 서로 인접한 모든 픽셀 쌍(수평 또는 수직 방향)에 대해, 픽셀 값의 차이를 제곱하여 모두 더한 값으로 계산한다. 여기서는 RGB 3개 입력 채널 각각에 대해 총변동 정규화를 계산한 뒤, 이를 모두 합산하고 총변동 가중치 $w_t$ 를 곱하여 최종 TV 손실을 정의한다:\n",
    "\n",
    "$L_{tv} = w_t \\times \\left(\\sum_{c=1}^3\\sum_{i=1}^{H-1}\\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2 + \\sum_{c=1}^3\\sum_{i=1}^{H}\\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\\right)$\n",
    "\n",
    "다음 셀에서 TV 손실 항을 정의하라. 전체 점수를 받으려면 구현에 루프를 포함하면 안 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tv_loss(img, tv_weight):\n",
    "    \"\"\"\n",
    "    Compute total variation loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - img: PyTorch Variable of shape (1, 3, H, W) holding an input image.\n",
    "    - tv_weight: Scalar giving the weight w_t to use for the TV loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Variable holding a scalar giving the total variation loss\n",
    "      for img weighted by tv_weight.\n",
    "    \"\"\"\n",
    "    # Your implementation should be vectorized and not require any loops!\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TV 손실 구현을 테스트하시오. 오차는 0.0001보다 작아야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tv_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    image_size =  192\n",
    "    tv_weight = 2e-2\n",
    "\n",
    "    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n",
    "    \n",
    "    student_output = tv_loss(content_img, tv_weight).cpu().data.numpy()\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Error is {:.3f}'.format(error))\n",
    "    \n",
    "tv_loss_test(answers['tv_out'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 것을 하나로 연결할 준비가 되었습니다!! (아래의 함수는 수정할 필요가 없어야 한다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
    "                   style_layers, style_weights, tv_weight, init_random = False):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    \n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "    - style_layers: list of layers to use for style loss\n",
    "    - style_weights: list of weights to use for each layer in style_layers\n",
    "    - tv_weight: weight of total variation regularization term\n",
    "    - init_random: initialize the starting image to uniform random noise\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract features for the content image\n",
    "    content_img = preprocess(PIL.Image.open(content_image), size=image_size)\n",
    "    feats = extract_features(content_img, cnn)\n",
    "    content_target = feats[content_layer].clone()\n",
    "\n",
    "    # Extract features for the style image\n",
    "    style_img = preprocess(PIL.Image.open(style_image), size=style_size)\n",
    "    feats = extract_features(style_img, cnn)\n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
    "\n",
    "    # Initialize output image to content image or nois\n",
    "    if init_random:\n",
    "        img = torch.Tensor(content_img.size()).uniform_(0, 1).type(dtype)\n",
    "    else:\n",
    "        img = content_img.clone().type(dtype)\n",
    "\n",
    "    # We do want the gradient computed on our image!\n",
    "    img.requires_grad_()\n",
    "    \n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 3.0\n",
    "    decayed_lr = 0.1\n",
    "    decay_lr_at = 180\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the img Torch tensor, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([img], lr=initial_lr)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].axis('off')\n",
    "    axarr[0].set_title('Content Source Img.')\n",
    "    axarr[1].set_title('Style Source Img.')\n",
    "    axarr[0].imshow(deprocess(content_img.cpu()))\n",
    "    axarr[1].imshow(deprocess(style_img.cpu()))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    \n",
    "    for t in range(200):\n",
    "        if t < 190:\n",
    "            img.data.clamp_(-1.5, 1.5)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        feats = extract_features(img, cnn)\n",
    "        \n",
    "        # Compute loss\n",
    "        c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "        s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
    "        t_loss = tv_loss(img, tv_weight) \n",
    "        loss = c_loss + s_loss + t_loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descents on our image values\n",
    "        if t == decay_lr_at:\n",
    "            optimizer = torch.optim.Adam([img], lr=decayed_lr)\n",
    "        optimizer.step()\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print('Iteration {}'.format(t))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(deprocess(img.data.cpu()))\n",
    "            plt.show()\n",
    "    print('Iteration {}'.format(t))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(deprocess(img.data.cpu()))\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멋진 이미지들을 만들어보자!\n",
    "\n",
    "아래의 세 가지 서로 다른 파라미터 세트에 대해 style_transfer를 실행해보라. 세 개의 셀을 모두 실행해야 한다. 자유롭게 파라미터를 추가해도 되지만, 제출하는 노트북에는 세 번째 파라미터 세트(starry night) 에 대한 스타일 트랜스퍼 결과를 반드시 포함해야 한다.\n",
    "\n",
    "파라미터 설명:\n",
    "\n",
    "* content_image: 콘텐츠 이미지의 파일 이름\n",
    "\n",
    "* style_image: 스타일 이미지의 파일 이름\n",
    "\n",
    "* image_size: 콘텐츠 이미지의 가장 작은 차원의 크기\n",
    "\n",
    "        콘텐츠 손실 및 생성 이미지에 사용됨\n",
    "\n",
    "* style_size: 스타일 이미지의 가장 작은 차원의 크기\n",
    "\n",
    "* content_layer: 콘텐츠 손실에 사용할 레이어\n",
    "\n",
    "* content_weight: 콘텐츠 손실 가중치\n",
    "\n",
    "        값이 클수록 결과 이미지가 콘텐츠 이미지와 더 유사해짐\n",
    "\n",
    "* style_layers: 스타일 손실에 사용할 레이어들의 리스트\n",
    "\n",
    "* style_weights: 각 스타일 레이어에 대응하는 가중치 목록\n",
    "\n",
    "        보통 더 이른 레이어에 더 높은 가중치를 부여\n",
    "\n",
    "        값을 키우면 스타일 이미지의 영향을 크게 받아 왜곡이 증가함\n",
    "\n",
    "* tv_weight: 총변동 정규화의 가중치\n",
    "\n",
    "        값을 키우면 결과 이미지가 더 매끄럽지만, 콘텐츠·스타일 정확도는 떨어짐\n",
    "\n",
    "### 실험 안내\n",
    "\n",
    "다음 세 개의 코드 셀은 하이퍼파라미터를 수정하지 말고 그대로 실행해야 한다.\n",
    "그 아래에서 파라미터를 복사하여 자유롭게 실험해보고, 결과 이미지가 어떻게 변하는지 확인해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Composition VII + Tubingen\n",
    "params1 = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/composition_vii.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 512,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 5e-2, \n",
    "    'style_layers' : (1, 4, 6, 7),\n",
    "    'style_weights' : (20000, 500, 12, 1),\n",
    "    'tv_weight' : 5e-2\n",
    "}\n",
    "\n",
    "style_transfer(**params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scream + Tubingen\n",
    "params2 = {\n",
    "    'content_image':'styles/tubingen.jpg',\n",
    "    'style_image':'styles/the_scream.jpg',\n",
    "    'image_size':192,\n",
    "    'style_size':224,\n",
    "    'content_layer':3,\n",
    "    'content_weight':3e-2,\n",
    "    'style_layers':[1, 4, 6, 7],\n",
    "    'style_weights':[200000, 800, 12, 1],\n",
    "    'tv_weight':2e-2\n",
    "}\n",
    "\n",
    "style_transfer(**params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Starry Night + Tubingen\n",
    "params3 = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/starry_night.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 192,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 6e-2,\n",
    "    'style_layers' : [1, 4, 6, 7],\n",
    "    'style_weights' : [300000, 1000, 15, 3],\n",
    "    'tv_weight' : 2e-2\n",
    "}\n",
    "\n",
    "style_transfer(**params3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Feature Inversion\n",
    "\n",
    "당신이 작성한 코드는 또 하나의 흥미로운 작업을 수행할 수 있다. 합성곱 신경망이 어떤 종류의 특징(feature)을 학습하는지 이해하기 위한 시도로, 최근 연구 [1]에서는 특징 표현(feature representation)으로부터 원본 이미지를 재구성하는 방법을 제안했다. 이는 사전학습된 네트워크로부터 이미지 그래디언트를 사용하는 것으로 구현할 수 있으며, 이는 우리가 위에서 수행한 방식과 동일하다(단지 두 종류의 특징 표현을 동시에 사용하지 않았을 뿐이다).\n",
    "\n",
    "이제 스타일 가중치를 모두 0으로 설정하고, 시작 이미지를 콘텐츠 원본 이미지 대신 무작위 노이즈(random noise)로 초기화하면, 콘텐츠 이미지의 특징 표현으로부터 이미지를 재구성할 수 있다. 초기에는 완전한 노이즈이지만, 최종적으로는 원본 이미지와 상당히 유사한 이미지가 얻어진다.\n",
    "\n",
    "(비슷하게, 콘텐츠 가중치를 0으로 설정하고 시작 이미지를 무작위 노이즈로 초기화하면, “텍스처 합성(texture synthesis)”을 처음부터 수행할 수도 있다. 하지만 여기서는 그것을 요구하지는 않는다.)\n",
    "\n",
    "아래 셀을 실행하여 feature inversion을 시도해보라.\n",
    "\n",
    "[1] Aravindh Mahendran, Andrea Vedaldi, Understanding Deep Image Representations by Inverting them, CVPR 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Inversion -- Starry Night + Tubingen\n",
    "params_inv = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/starry_night.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 192,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 6e-2,\n",
    "    'style_layers' : [1, 4, 6, 7],\n",
    "    'style_weights' : [0, 0, 0, 0], # we discard any contributions from style to the loss\n",
    "    'tv_weight' : 2e-2,\n",
    "    'init_random': True # we want to initialize our image to be random\n",
    "}\n",
    "\n",
    "style_transfer(**params_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
